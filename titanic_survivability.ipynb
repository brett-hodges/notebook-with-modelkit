{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating ModelKits into Jupyter Notebook Workflows: A Practical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kaggle competition, [Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic), issues a challenge to create a model that uses Titanic passenger data (name, age, price of ticket, etc) to try to predict who survived and who died.  \n",
    "\n",
    "While this Notebook does build out a solution to the problem posed, the primary goal isn't to create the best predictive model, but, instead, to demonstrate how to leverage [KitOps Modelkits](https://www.kitops.ml) within a machine learning workflow.\n",
    "\n",
    "And,though the current context applies to Jupyter Notebooks written in Python, the code provided could be used just as effectively in workflows existing outside of a Notebook environment, as well.  Also, the code's functionality could be easily reproduced in other programming languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If you haven't aready done so, [sign up for a free account with Jozu.ml](https://api.jozu.ml/signup)\n",
    "\n",
    "2. After you log into Jozu, add a new Repository named *\"titanic-survivability\"*, which we'll use in this Notebook.\n",
    "\n",
    "3. In the same directory as this Notebook--which we'll call the *Project directory*--create a `.env` file.\n",
    "\n",
    "4. Edit the `.env` file and add an entry for your **JOZU_USERNAME**, your **JOZU_PASSWORD** and your **JOZU_NAMESPACE** (aka your **Personal Organization** name). For example:\n",
    "```bash\n",
    "    JOZU_USERNAME=brett@jozu.org\n",
    "    JOZU_PASSWORD=my_password\n",
    "    JOZU_NAMESPACE=brett\n",
    "```\n",
    "5. Be sure to save the changes to your .env file before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Your Python Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This project was created using Python 3.12, but should work for Python versions >= 3.7.\n",
    "\n",
    "- We recommend using a Python or Conda virtual environment to isolate this project's code to prevent it from affecting the system-installed Python.\n",
    "\n",
    "- If you name your Python or Conda environment something other than \".venv\" or \"venv\", then be sure to add the name to the `.gitignore` file. *This step assumes you'll be using `git` for version control of this project.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Required Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (3.9.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (4.66.5)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (8.1.5)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (1.4.2)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (6.0.2)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (1.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 7)) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 7)) (8.28.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 7)) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in ./.venv/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 7)) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in ./.venv/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 7)) (3.0.13)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (2.18.0)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Working with Kitfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# Helper function to print the contents of the python dictionary object\n",
    "# representing the project's Kitfile\n",
    "def print_kitfile_contents(kitfile):\n",
    "    print('Kitfile Contents...')\n",
    "    print('===================\\n')\n",
    "    print(yaml.safe_dump(kitfile, sort_keys=False))\n",
    "\n",
    "# Helper function to read the project's Kitfile from disk into\n",
    "# a python dictionary object.\n",
    "# If the print flag is true, the Kitfile's contents are printed.\n",
    "def import_kitfile(print = True) -> dict:\n",
    "    # Path to kitfile template\n",
    "    kitfile_template = Path(\"template\") / \"Kitfile.template\"\n",
    "    # Open the Kitfile template\n",
    "    with open(kitfile_template, 'r') as file:\n",
    "        # Load the contents into a Python dictionary\n",
    "        kitfile = yaml.safe_load(file)\n",
    "    if print:\n",
    "        print_kitfile_contents(kitfile)\n",
    "    return kitfile\n",
    "\n",
    "# Helper function to export the python dictionary object \n",
    "# representing the project's Kitfile to disk.\n",
    "# If the print flag is true, the Kitfile's contents are printed.\n",
    "def export_kitfile(kitfile, print = True):\n",
    "    # Open the Kitfile \n",
    "    yaml.safe_dump(kitfile, open('Kitfile', 'w'), sort_keys=False)\n",
    "    if print:\n",
    "        print_kitfile_contents(kitfile)\n",
    "\n",
    "# Helper function to update the Kitfile's Datasets section with\n",
    "# details about the data files to include.\n",
    "# If the replace flag is True, the Datasets section will \n",
    "# be over-written; otherwise, the Datasets section will be\n",
    "# updated with the additional Datasets.\n",
    "# If the print flag is True, the contents of the updated\n",
    "# Kitfile are printed.\n",
    "def update_datasets_section(kitfile, datasets, replace = True, print = True):\n",
    "    if replace:\n",
    "        kitfile[\"datasets\"] = datasets \n",
    "    else:\n",
    "        kitfile[\"datasets\"].extend(datasets)\n",
    "    # save the updated Kitfile contents to disk\n",
    "    export_kitfile(kitfile, print)\n",
    "\n",
    "# Helper function to update the Kitfile's Package section with\n",
    "# details about the current ModelKit, including the tag name\n",
    "# to be used when pushing it, and the approximate timestamp\n",
    "# when the ModelKit is to be pushed.\n",
    "# If the print flag is True, the contents of the updated\n",
    "# Kitfile are printed.\n",
    "def update_package_section(kitfile, tag_name, print = True):\n",
    "    # Get the current UTC timestamp\n",
    "    current_utc_timestamp = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    package_section = kitfile[\"package\"]\n",
    "    description = package_section[\"description\"]\n",
    "    description += (\"\\nModelKit tag: \" + tag_name + \" pushed at: \" + current_utc_timestamp)\n",
    "    package_section[\"description\"] = description\n",
    "    # save the updated Kitfile contents to disk\n",
    "    export_kitfile(kitfile, print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Working with ModelKits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def kit_login(user: str, passwd: str, registry: str = \"jozu.ml\"):\n",
    "    subprocess.run([\"kit\", \"login\", registry, \"-u\", user, \"--password-stdin\"], input=passwd, text=True)\n",
    "\n",
    "def kit_logout(registry: str = \"jozu.ml\"):\n",
    "    subprocess.run([\"kit\", \"logout\", registry])\n",
    "\n",
    "def kit_pack(repo_path_with_tag: str):\n",
    "    subprocess.run([\"kit\", \"pack\", \".\", \"-t\", repo_path_with_tag])\n",
    "\n",
    "def kit_push(repo_path_with_tag):\n",
    "    subprocess.run([\"kit\", \"push\", repo_path_with_tag])\n",
    "\n",
    "def pack_and_push_modelkit(user: str, passwd: str, namespace: str, \n",
    "                           registry: str = \"jozu.ml\", tag:str = \"latest\"):\n",
    "    repo_path_with_tag = registry + \"/\" + namespace + \"/\" + \"titanic-survivability\" + \":\" + tag\n",
    "    kit_login(user, passwd, registry)\n",
    "    kit_pack(repo_path_with_tag)\n",
    "    kit_push(repo_path_with_tag)\n",
    "    kit_logout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the Project's Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "├── requirements.txt             # a list of python packages required for this project\n",
    "├── template\n",
    "│   └── Kitfile.template         # a base Kitfile used as a starting point\n",
    "├── docs\n",
    "│   ├── LICENSE                  # the license file \n",
    "│   └── README.md                # the project's README file\n",
    "├── titanic_survivability.ipynb  # this Jupyter Notebook\n",
    "├── Kitfile                      # the ModelKit's Kitfile to be updated via the Notebook's workflow\n",
    "└── data\n",
    "    ├── test.csv                 # the validation dataset\n",
    "    └── train.csv                # the training dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the ModelKit's Kitfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of every [ModelKit](https://kitops.ml/docs/modelkit/intro.html) is a [Kitfile](https://kitops.ml/docs/kitfile/format.html), a YAML-formatted configuration file. The Kitfile for this project has already been created with a base set of configuration details, but we'll update it as we progress through the workflow.\n",
    "\n",
    "Let's view the Kitfile's contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kitfile Contents...\n",
      "===================\n",
      "\n",
      "manifestVersion: 1.0\n",
      "package:\n",
      "  name: Titanic-Survivability-Predictor\n",
      "  version: 1.0.0\n",
      "  description: A project attempting to predict passenger survivability of the Titanic\n",
      "    Shipwreck.\n",
      "  authors:\n",
      "  - Jozu\n",
      "docs:\n",
      "- path: docs/README.md\n",
      "  description: Important notes about the project.\n",
      "- path: docs/LICENSE\n",
      "  description: The license for this ModelKit\n",
      "code:\n",
      "- path: requirements.txt\n",
      "  description: Python packages required by this example.\n",
      "  license: Apache-2.0\n",
      "- path: titanic_survivability.ipynb\n",
      "  description: Jupyter Notebook used to train, validate, optimize and export the model.\n",
      "  license: Apache-2.0\n",
      "- path: template/Kitfile.template\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kitfile = import_kitfile(print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `manifestVersion` and `package` sections primarily include metadata about the ModelKit.  The `docs` and `code` sections contain references to their respective project files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll load the unprocessed data into two separate datasets:  one to be used for model training, and the other to be used for model testing. Once the unprocessed data sets have been collated, their paths will be added to the ModelKit's Kitfile, and the current state of the ModelKit will be packed and pushed to Jozu Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# load the titanic data \n",
    "train_data, test_data = [pd.read_csv(Path(\"data\") / filename) for filename in (\"train.csv\", \"test.csv\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the Kitfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the datasets to the Kitfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data files loaded, now's a good time to update our ModelKit's Kitfile with the configuration details for the `datasets` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the test and train data sets to the Kitfile\n",
    "training_data_info = {\n",
    "    \"name\": \"training\",\n",
    "    \"path\": \"data/train.csv\",\n",
    "    \"description\": \"Unprocessed data to be used for model training.\",\n",
    "    \"license\": \"Apache-2.0\"\n",
    "}\n",
    "\n",
    "testing_data_info = {\n",
    "    \"name\": \"testing\",\n",
    "    \"path\": \"data/test.csv\",\n",
    "    \"description\": \"Unprocessed data to be used for model testing.\",\n",
    "    \"license\": \"Apache-2.0\"\n",
    "}\n",
    "\n",
    "datasets_info = [\n",
    "    training_data_info,\n",
    "    testing_data_info\n",
    "]\n",
    "\n",
    "update_datasets_section(kitfile, datasets_info, print = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let' update the Kitfile's Package `Description` will be updated to include the `tag` name of the ModelKit's version, along with the current UTC timestamp of this update for future reference.  The contents of the updated Kitfile will be then be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kitfile Contents...\n",
      "===================\n",
      "\n",
      "manifestVersion: 1.0\n",
      "package:\n",
      "  name: Titanic-Survivability-Predictor\n",
      "  version: 1.0.0\n",
      "  description: 'A project attempting to predict passenger survivability of the Titanic\n",
      "    Shipwreck.\n",
      "\n",
      "    ModelKit tag: collated-data-v1 pushed at: 2024-10-21 21:05:15 UTC'\n",
      "  authors:\n",
      "  - Jozu\n",
      "docs:\n",
      "- path: docs/README.md\n",
      "  description: Important notes about the project.\n",
      "- path: docs/LICENSE\n",
      "  description: The license for this ModelKit\n",
      "code:\n",
      "- path: requirements.txt\n",
      "  description: Python packages required by this example.\n",
      "  license: Apache-2.0\n",
      "- path: titanic_survivability.ipynb\n",
      "  description: Jupyter Notebook used to train, validate, optimize and export the model.\n",
      "  license: Apache-2.0\n",
      "- path: template/Kitfile.template\n",
      "datasets:\n",
      "- name: training\n",
      "  path: data/train.csv\n",
      "  description: Unprocessed data to be used for model training.\n",
      "  license: Apache-2.0\n",
      "- name: testing\n",
      "  path: data/test.csv\n",
      "  description: Unprocessed data to be used for model testing.\n",
      "  license: Apache-2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#construct the tag name for this ModelKit's version\n",
    "tag_name = \"collated-data-v1\"\n",
    "\n",
    "update_package_section(kitfile, tag_name, print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push this ModelKit's Version to Jozu Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pack and push the ModelKit version tagged as `collated-data-v1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log in successful\n",
      "Saved configuration: sha256:cc78b2b9f7bb1cdae367b0c27bffae0a3c75004a411ca679ae4f34f234ba5a34\n",
      "Already saved code layer: sha256:9de9f1238dd7a623c617cbc14e7f5eada59d28085909e9cf57279ce0ebd6a5ee\n",
      "Saved code layer: sha256:63ea171856317029b88dd3c0d291aea7a415ae1ae6bfd188549eea0412f9381e\n",
      "Already saved code layer: sha256:220109bcd4dc8be72b9228aefb24f69239c3308297cb4a4806705c002e910e3c\n",
      "Already saved dataset layer: sha256:8af74261fda984270cc30a58277f28c3435de603ba2722e160528a0fab6d1127\n",
      "Already saved dataset layer: sha256:cbd1c99ae96851ca944aeda77e1e8338c00fd639b9cdcdbe42044ec99352cb79\n",
      "Already saved docs layer: sha256:7de63cef90f75c857e373367403ad0a23143068b62e5b725c02f845043df6c8c\n",
      "Already saved docs layer: sha256:0b45ae1c1fe73e1bcc537625392baeb70253f551ab2b3ee384b214fbf82c2be7\n",
      "Saved manifest to storage: sha256:1c99095ad1b876dba3a2178f72c0e1c4768b33fe9714e5681d998114a7c8f053\n",
      "Model saved: sha256:1c99095ad1b876dba3a2178f72c0e1c4768b33fe9714e5681d998114a7c8f053\n",
      "Pushing jozu.ml/brett/titanic-survivability:collated-data-v1\n",
      "Pushed sha256:1c99095ad1b876dba3a2178f72c0e1c4768b33fe9714e5681d998114a7c8f053\n",
      "Successfully logged out from jozu.ml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# load the login credentials to Jozu.ml taken from environment variables stored in the .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# pack and push the 'collated-data-v1` ModelKit version\n",
    "pack_and_push_modelkit(user = os.getenv(\"JOZU_USERNAME\"), \n",
    "                       passwd = os.getenv(\"JOZU_PASSWORD\"), \n",
    "                       namespace = os.getenv(\"JOZU_NAMESPACE\"),\n",
    "                       registry = \"jozu.ml\",\n",
    "                       tag = tag_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "5            6         0       3   \n",
       "6            7         0       1   \n",
       "7            8         0       3   \n",
       "8            9         1       3   \n",
       "9           10         1       2   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                   Moran, Mr. James    male   NaN      0   \n",
       "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  \n",
       "5      0            330877   8.4583   NaN        Q  \n",
       "6      0             17463  51.8625   E46        S  \n",
       "7      1            349909  21.0750   NaN        S  \n",
       "8      2            347742  11.1333   NaN        S  \n",
       "9      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attributes have the following meaning:\n",
    "* **PassengerId**: a unique identifier for each passenger\n",
    "* **Survived**: that's the target, 0 means the passenger did not survive, while 1 means he/she survived.\n",
    "* **Pclass**: passenger class.\n",
    "* **Name**, **Sex**, **Age**: self-explanatory\n",
    "* **SibSp**: how many siblings & spouses of the passenger aboard the Titanic.\n",
    "* **Parch**: how many children & parents of the passenger aboard the Titanic.\n",
    "* **Ticket**: ticket id\n",
    "* **Fare**: price paid (in pounds)\n",
    "* **Cabin**: passenger's cabin number\n",
    "* **Embarked**: where the passenger embarked the Titanic\n",
    "\n",
    "We want to train a model that predicts which passengers **Survived** based on the values in the other attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much data is missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "- The **PassengerID** attribute may be used as the dataset's index\n",
    "- The **Name** and **Ticket** attributes may have some value, but they will be a bit tricky to convert into useful numbers that a model can consume. So for now, we will ignore them.\n",
    "- The **SibSp** and **Parch** attributes may be added to create the **NumRelatives** attribute\n",
    "- About 77% of the **Cabin** values are null, so we'll ignore that column, as well.\n",
    "- About 19% of the **Age** values are null, but we can replace those values with mean of the k-nearest neighbors.\n",
    "- Two of the **Embarked** values are empty, but we can replace those values with the most common value in that column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explicitly set the **PassengerId** column as the index column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.set_index(\"PassengerId\")\n",
    "test_data = test_data.set_index(\"PassengerId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the new attribute **NumRelatives** by adding the values from **SibSp** and **Parch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"NumRelatives\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\n",
    "test_data[\"NumRelatives\"] = test_data[\"SibSp\"] + test_data[\"Parch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Run the Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the pipeline for numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        (\"imputer\", KNNImputer(n_neighbors=2)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And continuing with the pipeline for the categorical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        (\"ordinal_encoder\", OrdinalEncoder()),    \n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"cat_encoder\", OneHotEncoder(sparse_output=False)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's join the numerical and categorical pipelines and run them agains the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = [\"Age\", \"NumRelatives\", \"Fare\"]\n",
    "cat_attribs = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "\n",
    "preprocess_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "    ])\n",
    "\n",
    "X_train = preprocess_pipeline.fit_transform(train_data)\n",
    "X_test = preprocess_pipeline.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data[\"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save the processed datasets to disk\n",
    "np.save(Path(\"data\") / \"processed_X_train.npy\", X_train)\n",
    "np.save(Path(\"data\") / \"processed_X_test.npy\", X_test)\n",
    "y_train.to_csv(Path(\"data\") / \"processed_y_train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the Kitfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to update the Kitfile's `datasets` section to include the new processed data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the test and train data sets to the Kitfile\n",
    "training_data_info = {\n",
    "    \"name\": \"processed_training\",\n",
    "    \"path\": \"data/processed_X_train.npy\",\n",
    "    \"description\": \"Processed data to be used for model training.\",\n",
    "    \"license\": \"Apache-2.0\"\n",
    "}\n",
    "\n",
    "testing_data_info = {\n",
    "    \"name\": \"processed_testing\",\n",
    "    \"path\": \"data/processed_X_test.npy\",\n",
    "    \"description\": \"Processed data to be used for model testing.\",\n",
    "    \"license\": \"Apache-2.0\"\n",
    "}\n",
    "\n",
    "label_data_info = {\n",
    "    \"name\": \"processed_labels\",\n",
    "    \"path\": \"data/processed_y_train.csv\",\n",
    "    \"description\": \"Processed labesl to be used for model training.\",\n",
    "    \"license\": \"Apache-2.0\"\n",
    "}\n",
    "\n",
    "datasets_info = [\n",
    "    training_data_info,\n",
    "    testing_data_info,\n",
    "    label_data_info\n",
    "]\n",
    "\n",
    "update_datasets_section(kitfile, datasets_info, replace = False, print = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we need to update the Kitfile's `package` section with this ModelKit version's tag name and approximate timestamp that it was updated. Then display the updated Kitfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kitfile Contents...\n",
      "===================\n",
      "\n",
      "manifestVersion: 1.0\n",
      "package:\n",
      "  name: Titanic-Survivability-Predictor\n",
      "  version: 1.0.0\n",
      "  description: 'A project attempting to predict passenger survivability of the Titanic\n",
      "    Shipwreck.\n",
      "\n",
      "    ModelKit tag: collated-data-v1 pushed at: 2024-10-21 21:05:15 UTC\n",
      "\n",
      "    ModelKit tag: processed-data-v5 pushed at: 2024-10-21 21:07:28 UTC'\n",
      "  authors:\n",
      "  - Jozu\n",
      "docs:\n",
      "- path: docs/README.md\n",
      "  description: Important notes about the project.\n",
      "- path: docs/LICENSE\n",
      "  description: The license for this ModelKit\n",
      "code:\n",
      "- path: requirements.txt\n",
      "  description: Python packages required by this example.\n",
      "  license: Apache-2.0\n",
      "- path: titanic_survivability.ipynb\n",
      "  description: Jupyter Notebook used to train, validate, optimize and export the model.\n",
      "  license: Apache-2.0\n",
      "- path: template/Kitfile.template\n",
      "datasets:\n",
      "- name: training\n",
      "  path: data/train.csv\n",
      "  description: Unprocessed data to be used for model training.\n",
      "  license: Apache-2.0\n",
      "- name: testing\n",
      "  path: data/test.csv\n",
      "  description: Unprocessed data to be used for model testing.\n",
      "  license: Apache-2.0\n",
      "- name: processed_training\n",
      "  path: data/processed_X_train.npy\n",
      "  description: Processed data to be used for model training.\n",
      "  license: Apache-2.0\n",
      "- name: processed_testing\n",
      "  path: data/processed_X_test.npy\n",
      "  description: Processed data to be used for model testing.\n",
      "  license: Apache-2.0\n",
      "- name: processed_labels\n",
      "  path: data/processed_y_train.csv\n",
      "  description: Processed labesl to be used for model training.\n",
      "  license: Apache-2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#construct the tag name for this ModelKit's version\n",
    "tag_name = \"processed-data-v5\"\n",
    "\n",
    "update_package_section(kitfile, tag_name, print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data processed, we can pack and push the ModelKit's latest state  with the tag name: `processed-data-v5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log in successful\n",
      "Saved configuration: sha256:d30417b7df6486d5ac700bb207196fc224b924f3fe30ab171310636f127d24b7\n",
      "Already saved code layer: sha256:9de9f1238dd7a623c617cbc14e7f5eada59d28085909e9cf57279ce0ebd6a5ee\n",
      "Already saved code layer: sha256:63ea171856317029b88dd3c0d291aea7a415ae1ae6bfd188549eea0412f9381e\n",
      "Already saved code layer: sha256:220109bcd4dc8be72b9228aefb24f69239c3308297cb4a4806705c002e910e3c\n",
      "Already saved dataset layer: sha256:8af74261fda984270cc30a58277f28c3435de603ba2722e160528a0fab6d1127\n",
      "Already saved dataset layer: sha256:cbd1c99ae96851ca944aeda77e1e8338c00fd639b9cdcdbe42044ec99352cb79\n",
      "Saved dataset layer: sha256:b977d0496b3ecc638e56502bbba28dcbf14534ba8d21eae27358c9f0207a83af\n",
      "Saved dataset layer: sha256:2b89b253b6ea4eec1f505d6953fc8ad4275f9c10c8eaa3ea46ec42c29bc8c2b5\n",
      "Already saved dataset layer: sha256:8b10202d2fc9010b880469a826a92bf9b0c10da52eba303d654cbb3ba5efdbeb\n",
      "Already saved docs layer: sha256:7de63cef90f75c857e373367403ad0a23143068b62e5b725c02f845043df6c8c\n",
      "Already saved docs layer: sha256:0b45ae1c1fe73e1bcc537625392baeb70253f551ab2b3ee384b214fbf82c2be7\n",
      "Saved manifest to storage: sha256:93b6022960e96acc2980265de9bc39943540ddc49d660c6ec5d70a5e655a4ef5\n",
      "Model saved: sha256:93b6022960e96acc2980265de9bc39943540ddc49d660c6ec5d70a5e655a4ef5\n",
      "Pushing jozu.ml/brett/titanic-survivability:processed-data-v5\n",
      "Pushed sha256:93b6022960e96acc2980265de9bc39943540ddc49d660c6ec5d70a5e655a4ef5\n",
      "Successfully logged out from jozu.ml\n"
     ]
    }
   ],
   "source": [
    "# load the login credentials to Jozu.ml taken from environment variables stored in the .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# pack and push the 'processed-data-v5` ModelKit version\n",
    "pack_and_push_modelkit(user = os.getenv(\"JOZU_USERNAME\"), \n",
    "                       passwd = os.getenv(\"JOZU_PASSWORD\"), \n",
    "                       namespace = os.getenv(\"JOZU_NAMESPACE\"),\n",
    "                       registry = \"jozu.ml\",\n",
    "                       tag = tag_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training a `RandomForestClassifier` to predict the survivability of the passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model trained, let's use it to make predictions on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the mean accuracy of 10 cross-validation folds to get an idea of how good our model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rfc_scores = cross_val_score(rfc, X_train, y_train, cv=10)\n",
    "rfc_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs at about 81.8% accuracy.  There are a number of things we could do to try to improve our prediction accuracy--such as *feature engineering*, *trying different types of models*, and *optimizing our models' parmameters*--but, for the purpose of this exercise, we'll assume we're ready to move our Model into production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's export our trained `RandomForestClassifier` model to the joblib-formatted file named, **model.joblib**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "artifact_filename = 'model.joblib'\n",
    "\n",
    "# Save model artifact to local filesystem (doesn't persist)\n",
    "model_path = Path() / \"model\" / artifact_filename\n",
    "joblib.dump(rfc, model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the Kitfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model exported, let's update our ModelKit's Kitfile with the configuration details for the `model` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'images' folder to the 'docs' section of the Kitfile\n",
    "model_info = {\n",
    "    \"name\": \"titanic-survivability-predictor\",\n",
    "    \"path\": str(model_path),\n",
    "    \"description\": \"RandomForestClassifier\",\n",
    "    \"framework\": \"joblib\",\n",
    "    \"license\": \"Apache-2.0\",\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "kitfile[\"model\"] = model_info\n",
    "\n",
    "# save the updated Kitfile contents to disk\n",
    "export_kitfile(kitfile)\n",
    "\n",
    "# reload the Kitfile from disk and display the contents\n",
    "# to make sure it was persisited correctly\n",
    "print_kitfile_contents(kitfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Pushing the ModelKit to Jozu Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can pack and push our final ModelKit to Jozu Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# the login credentials to Jozu.ml taken from environment variables stored in the .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "pack_and_push_modelkit(user = os.getenv(\"JOZU_USERNAME\"), \n",
    "                       passwd = os.getenv(\"JOZU_PASSWORD\"), \n",
    "                       namespace = os.getenv(\"JOZU_NAMESPACE\"),\n",
    "                       registry = \"jozu.ml\",\n",
    "                       tag = \"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log back into [Jozu Hub](https://jozu.ml) in your browser, click on *My Repositories* and you should see your ModelKit tagged as \"latest\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
