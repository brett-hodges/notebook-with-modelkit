{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating ModelKits into Jupyter Notebook Workflows: A Practical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kaggle competition, [Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic), issues a challenge to create a model that uses Titanic passenger data (name, age, price of ticket, etc) to try to predict who survived and who died.  \n",
    "\n",
    "While this Notebook does build out a solution to the problem posed, the primary goal isn't to create the best predictive model, but, instead, to demonstrate how to leverage [KitOps Modelkits](https://www.kitops.ml) within a machine learning workflow.\n",
    "\n",
    "And, though the current context applies to Jupyter Notebooks written in Python, the code provided could be used just as effectively in workflows existing outside of a Notebook environment, as well.  Also, the code's functionality could be easily reproduced in other programming languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before You Begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If you haven't already done so, [sign up for a free account with Jozu.ml](https://api.jozu.ml/signup)\n",
    "\n",
    "2. After you log into Jozu, add a new Repository named *\"titanic-survivability\"*, which we'll use in this Notebook.\n",
    "\n",
    "3. In the same directory as this Notebook--which we'll call the *Project directory*--create a `.env` file.\n",
    "\n",
    "4. Edit the `.env` file and add an entry for your **JOZU_USERNAME**, your **JOZU_PASSWORD** and your **JOZU_NAMESPACE** (aka your **Personal Organization** name). For example:\n",
    "```bash\n",
    "    JOZU_USERNAME=brett@jozu.org\n",
    "    JOZU_PASSWORD=my_password\n",
    "    JOZU_NAMESPACE=brett\n",
    "```\n",
    "5. Be sure to save the changes to your .env file before continuing.\n",
    "\n",
    "6. Install the [Kit CLI](https://kitops.ml/docs/cli/installation.html) on the machine where the notebook will run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Your Python Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This project was created using Python 3.12, but should work for Python versions >= 3.7.\n",
    "\n",
    "- We recommend using a Python or Conda virtual environment to isolate this project's code to prevent it from affecting the system-installed Python.\n",
    "\n",
    "- If you name your Python or Conda environment something other than \".venv\" or \"venv\", then be sure to add the name to the `.gitignore` file. *This step assumes you'll be using `git` for version control of this project.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Required Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (3.9.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (4.66.5)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (8.1.5)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (1.4.2)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (6.0.2)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (1.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 7)) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 7)) (8.28.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 7)) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in ./.venv/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 7)) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in ./.venv/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 7)) (3.0.13)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (2.18.0)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 7)) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Collated Datasets from Jozu Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull the `collated-data-v1` ModelKit Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tagged ModelKit version contains the unprocessed data split into two separate datasets:  one to be used for model training, and the other to be used for model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log in successful\n",
      "Unpacking to /Users/brett/Projects/Demos/notebook-with-modelkit\n",
      "Unpacking config to /Users/brett/Projects/Demos/notebook-with-modelkit/Kitfile\n",
      "Unpacking dataset training to data/train.csv\n",
      "Unpacking dataset testing to data/test.csv\n",
      "Unpacking docs to docs/README.md\n",
      "Unpacking docs to docs/LICENSE\n",
      "Successfully logged out from jozu.ml\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from kitfile_helpers import import_kitfile\n",
    "from modelkit_helpers import unpack_collated_data_modelkit\n",
    "import os\n",
    "\n",
    "#construct the tag name for this ModelKit's version\n",
    "tag_name = \"collated-data-v1\"\n",
    "\n",
    "# load the login credentials to Jozu.ml taken from environment variables stored in the .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# unpack the 'collated-data-v1` ModelKit version to the local machine\n",
    "unpack_collated_data_modelkit(user = os.getenv(\"JOZU_USERNAME\"), \n",
    "                            passwd = os.getenv(\"JOZU_PASSWORD\"), \n",
    "                            tag = tag_name)\n",
    "\n",
    "kitfile = import_kitfile(print = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the Project's Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "├── Kitfile                      # the ModelKit's Kitfile to be updated via the Notebook's workflow\n",
    "└── data\n",
    "    ├── test.csv                 # the validation dataset\n",
    "    └── train.csv                # the training dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the ModelKit's Kitfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of every [ModelKit](https://kitops.ml/docs/modelkit/intro.html) is a [Kitfile](https://kitops.ml/docs/kitfile/format.html), a YAML-formatted configuration file. The Kitfile for this project has already been created with a base set of configuration details, but we'll update it as we progress through the workflow.\n",
    "\n",
    "Let's view the Kitfile's contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitfile = import_kitfile(print = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `manifestVersion` and `package` sections primarily include metadata about the ModelKit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Collated Datasets from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# load the titanic data \n",
    "train_data, test_data = [pd.read_csv(Path(\"data\") / filename) for filename in (\"train.csv\", \"test.csv\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attributes have the following meaning:\n",
    "* **PassengerId**: a unique identifier for each passenger\n",
    "* **Survived**: that's the target, 0 means the passenger did not survive, while 1 means he/she survived.\n",
    "* **Pclass**: passenger class.\n",
    "* **Name**, **Sex**, **Age**: self-explanatory\n",
    "* **SibSp**: how many siblings & spouses of the passenger aboard the Titanic.\n",
    "* **Parch**: how many children & parents of the passenger aboard the Titanic.\n",
    "* **Ticket**: ticket id\n",
    "* **Fare**: price paid (in pounds)\n",
    "* **Cabin**: passenger's cabin number\n",
    "* **Embarked**: where the passenger embarked the Titanic\n",
    "\n",
    "We want to train a model that predicts which passengers **Survived** based on the values in the other attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much data is missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "- The **PassengerID** attribute may be used as the dataset's index\n",
    "- The **Name** and **Ticket** attributes may have some value, but they will be a bit tricky to convert into useful numbers that a model can consume. So for now, we will ignore them.\n",
    "- The **SibSp** and **Parch** attributes may be added to create the **NumRelatives** attribute\n",
    "- About 77% of the **Cabin** values are null, so we'll ignore that column, as well.\n",
    "- About 19% of the **Age** values are null, but we can replace those values with mean of the k-nearest neighbors.\n",
    "- Two of the **Embarked** values are empty, but we can replace those values with the most common value in that column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explicitly set the **PassengerId** column as the index column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.set_index(\"PassengerId\")\n",
    "test_data = test_data.set_index(\"PassengerId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the new attribute **NumRelatives** by adding the values from **SibSp** and **Parch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"NumRelatives\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\n",
    "test_data[\"NumRelatives\"] = test_data[\"SibSp\"] + test_data[\"Parch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Run the Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the pipeline for numerical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        (\"imputer\", KNNImputer(n_neighbors=2)),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And continuing with the pipeline for the categorical attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        (\"ordinal_encoder\", OrdinalEncoder()),    \n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"cat_encoder\", OneHotEncoder(sparse_output=False)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's join the numerical and categorical pipelines and run them agains the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = [\"Age\", \"NumRelatives\", \"Fare\"]\n",
    "cat_attribs = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "\n",
    "preprocess_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "    ])\n",
    "\n",
    "X_train = preprocess_pipeline.fit_transform(train_data)\n",
    "X_test = preprocess_pipeline.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data[\"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save the processed datasets to disk\n",
    "np.save(Path(\"data\") / \"processed_X_train.npy\", X_train)\n",
    "np.save(Path(\"data\") / \"processed_X_test.npy\", X_test)\n",
    "y_train.to_csv(Path(\"data\") / \"processed_y_train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the Kitfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to update the Kitfile's `datasets` section to include the new processed data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kitfile_helpers import update_datasets_section\n",
    "\n",
    "# add the test and train data sets to the Kitfile\n",
    "training_data_info = {\n",
    "    \"name\": \"processed_training\",\n",
    "    \"path\": \"data/processed_X_train.npy\",\n",
    "    \"description\": \"Processed data to be used for model training.\",\n",
    "    \"license\": \"Apache-2.0\"\n",
    "}\n",
    "\n",
    "testing_data_info = {\n",
    "    \"name\": \"processed_testing\",\n",
    "    \"path\": \"data/processed_X_test.npy\",\n",
    "    \"description\": \"Processed data to be used for model testing.\",\n",
    "    \"license\": \"Apache-2.0\"\n",
    "}\n",
    "\n",
    "label_data_info = {\n",
    "    \"name\": \"processed_labels\",\n",
    "    \"path\": \"data/processed_y_train.csv\",\n",
    "    \"description\": \"Processed labesl to be used for model training.\",\n",
    "    \"license\": \"Apache-2.0\"\n",
    "}\n",
    "\n",
    "datasets_info = [\n",
    "    training_data_info,\n",
    "    testing_data_info,\n",
    "    label_data_info\n",
    "]\n",
    "\n",
    "update_datasets_section(kitfile, datasets_info, replace = False, print = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to update the Kitfile's `code` section to include the Jupyter Notebook and related code files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kitfile_helpers import update_code_section\n",
    "\n",
    "# add the Jupyter Notebook\n",
    "notebook = {\n",
    "    \"path\": \"titanic_survivability.ipynb\",\n",
    "    \"description\": \"Jupyter notebook.\",\n",
    "    \"license\": \"Apache-2.0\"\n",
    "}\n",
    "\n",
    "requirements = {\n",
    "    \"path\": \"requirements.txt\",\n",
    "    \"description\": \"Required packages for the notebook.\",\n",
    "    \"license\": \"Apache-2.0\"\n",
    "}\n",
    "\n",
    "kitfile_template = {\n",
    "    \"path\": \"template/Kitfile.template\",\n",
    "    \"description\": \"Template for the Kitfile.\",\n",
    "    \"license\": \"Apache-2.0\"\n",
    "}\n",
    "\n",
    "kitfile_helpers = {\n",
    "    \"path\": \"kitfile_helpers.py\",\n",
    "    \"description\": \"Python helper functions for authoring Kitfiles.\",\n",
    "    \"license\": \"Apache-2.0\"\n",
    "}\n",
    "\n",
    "modelkit_helpers = {\n",
    "    \"path\": \"modelkit_helpers.py\",\n",
    "    \"description\": \"Python helper functions for performing ModelKit actions.\",\n",
    "    \"license\": \"Apache-2.0\"\n",
    "}\n",
    "\n",
    "gitignore = {\n",
    "    \"path\": \".gitignore\",\n",
    "    \"description\": \"gitignore file.\",\n",
    "    \"license\": \"Apache-2.0\"\n",
    "}\n",
    "\n",
    "code_info = [\n",
    "    notebook,\n",
    "    requirements,\n",
    "    kitfile_template,\n",
    "    kitfile_helpers,\n",
    "    modelkit_helpers,\n",
    "    gitignore\n",
    "]\n",
    "\n",
    "update_code_section(kitfile, code_info, replace = True, print = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push this ModelKit's Version to Jozu Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we need to update the Kitfile's `package` section with this ModelKit version's tag name and approximate timestamp that it was updated. Then display the updated Kitfile.\n",
    "\n",
    "Next, let's update the Kitfile's Package `Description` to include the tag name, `processed-data-v5`,  along with the current UTC timestamp of this update for future reference.\n",
    "\n",
    "Then, we'll pack and push the ModelKit version tagged as `processed-data-v5` to Jozu Hub.\n",
    "\n",
    "Finally, we'll print the contents of the updated Kitfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kitfile_helpers import update_package_section\n",
    "from modelkit_helpers import pack_and_push_modelkit\n",
    "\n",
    "#construct the tag name for this ModelKit's version\n",
    "tag_name = \"processed-data-v5\"\n",
    "\n",
    "update_package_section(kitfile, tag_name, print = False)\n",
    "\n",
    "# load the login credentials to Jozu.ml taken from environment variables stored in the .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# pack and push the 'processed-data-v5` ModelKit version\n",
    "pack_and_push_modelkit(user = os.getenv(\"JOZU_USERNAME\"), \n",
    "                       passwd = os.getenv(\"JOZU_PASSWORD\"), \n",
    "                       namespace = os.getenv(\"JOZU_NAMESPACE\"),\n",
    "                       registry = \"jozu.ml\",\n",
    "                       tag = tag_name)\n",
    "\n",
    "kitfile = import_kitfile(print = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data processed, we can pack and push the ModelKit's latest state  with the tag name: `processed-data-v5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training a `RandomForestClassifier` to predict the survivability of the passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model trained, let's use it to make predictions on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the mean accuracy of 10 cross-validation folds to get an idea of how good our model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rfc_scores = cross_val_score(rfc, X_train, y_train, cv=10)\n",
    "rfc_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs at about 81.8% accuracy.  There are a number of things we could do to try to improve our prediction accuracy--such as *feature engineering*, *trying different types of models*, and *optimizing our models' parmameters*--but, for the purpose of this exercise, we'll assume we're ready to move our Model into production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's export our trained `RandomForestClassifier` model to the joblib-formatted file named, **model.joblib**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "artifact_filename = 'model.joblib'\n",
    "\n",
    "# Save model artifact to local filesystem (doesn't persist)\n",
    "model_path = Path() / \"model\" / artifact_filename\n",
    "joblib.dump(rfc, model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the Kitfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model exported, let's update our ModelKit's Kitfile with the configuration details for the `model` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kitfile_helpers import update_model_section\n",
    "\n",
    "# add the `model` section to the kitfile\n",
    "model_info = {\n",
    "    \"name\": \"titanic-survivability-predictor\",\n",
    "    \"path\": str(model_path),\n",
    "    \"description\": \"RandomForestClassifier\",\n",
    "    \"framework\": \"joblib\",\n",
    "    \"license\": \"Apache-2.0\",\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "update_model_section(kitfile, model_info, print = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the ModelKit to Jozu Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can tag this ModelKit version as `trained_model_v2`, pack and push it to Jozu Hub, and print the updated Kitfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct the tag name for this ModelKit's version\n",
    "tag_name = \"trained_model_v2\"\n",
    "\n",
    "update_package_section(kitfile, tag_name, print = False)\n",
    "\n",
    "# load the login credentials to Jozu.ml taken from environment variables stored in the .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# pack and push the 'processed-data-v5` ModelKit version\n",
    "pack_and_push_modelkit(user = os.getenv(\"JOZU_USERNAME\"), \n",
    "                       passwd = os.getenv(\"JOZU_PASSWORD\"), \n",
    "                       namespace = os.getenv(\"JOZU_NAMESPACE\"),\n",
    "                       registry = \"jozu.ml\",\n",
    "                       tag = tag_name)\n",
    "\n",
    "kitfile = import_kitfile(print = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view your tagged ModelKits:\n",
    "1. Log back into [Jozu Hub](https://jozu.ml) in your browser\n",
    "2. Click on *My Repositories*\n",
    "3. Click on *titanic-survivability*\n",
    "4. Your tagged ModelKits will be displayed.\n",
    "\n",
    "You can expand each one and click the \"MORE DETAILS\" link to view information about their contents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
